{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20bc6003",
   "metadata": {},
   "source": [
    "# Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4e2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e7577",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8b69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOKEN_SPAM_PROB_FILE = 'SpamData/03_Testing/prob-spam.txt'\n",
    "TOKEN_HAM_PROB_FILE = 'SpamData/03_Testing/prob-nonspam.txt'\n",
    "TOKEN_ALL_PROB_FILE = 'SpamData/03_Testing/prob-all-tokens.txt'\n",
    "\n",
    "TEST_FEATURE_MATRIX = 'SpamData/03_Testing/test-features.txt' \n",
    "TEST_TARGET_FILE = 'SpamData/03_Testing/test-target.txt' \n",
    "\n",
    "VOCAB_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a69bc0",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5bd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.loadtxt(TEST_FEATURE_MATRIX, delimiter=' ')\n",
    "y_test = np.loadtxt(TEST_TARGET_FILE, delimiter=' ')\n",
    "\n",
    "prob_token_spam = np.loadtxt(TOKEN_SPAM_PROB_FILE, delimiter=' ')\n",
    "prob_token_ham = np.loadtxt(TOKEN_HAM_PROB_FILE, delimiter=' ')\n",
    "prob_all_tokens = np.loadtxt(TOKEN_ALL_PROB_FILE, delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5814e",
   "metadata": {},
   "source": [
    "# Calculating the Joint Probability\n",
    "\n",
    "### The Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3521b252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1724, 2500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cfe537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_token_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c84bac7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1724,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dot(prob_token_spam).shape # I can't do it this way => prob_token_spam.dot(X_test).shape because \n",
    "# the values are not aligned [(1724, 2500)] * [(2500,)] != [(2500,)] * [(1724, 2500)]. where the former is\n",
    "# the correct way of doing it since the col of the first matrix(2500 )is aligned properly with the row of the second matrix(2500 )\n",
    "# also this is the right way to do it since we are multiplying the column of the 2d array by the row of the 1d array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23717e4",
   "metadata": {},
   "source": [
    "## Set the Prior\n",
    "\n",
    "## $$ P(Spam \\, | \\, X) =  \\frac{P(X \\, | \\, Spam \\,) \\, P(Spam)} {P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb893a",
   "metadata": {},
   "source": [
    "<p> A prior is a belief or guess about a quantity. In our case, we already have a calculated \"guess\", which is the probability of spam = 0.3109 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75cf70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROB_SPAM = 0.3109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c15f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.40768141,  -5.25375622,  -4.99015865, ...,  -9.79171765,\n",
       "        -9.52935339, -10.70800838])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(prob_token_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed9864",
   "metadata": {},
   "source": [
    "## Joint Probability in Log Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3af7e8b0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.26609396,  2.15741368, 20.58593663, 17.73868148, 20.49790994])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_log_spam = X_test.dot(np.log(prob_token_spam) - np.log(prob_all_tokens)) + np.log(PROB_SPAM)\n",
    "joint_log_spam[:5]\n",
    "# since we are using log format instead of dividing by p(X) we'll subtract the prob_token_spam from \n",
    "# prob_all_tokens and also we won't be multiplying against the p(Spam) rather we'll be adding it up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5db947",
   "metadata": {},
   "source": [
    "## $$ P(Ham \\, | \\, X) =  \\frac{P(X \\, | \\, Ham \\,) \\, (1 - P(Spam) )} {P(X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "944ab875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-60.9615197 , -11.00803242, -37.96485145, -59.12449324,\n",
       "       -53.78250218])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_log_ham = X_test.dot(np.log(prob_token_ham) - np.log(prob_all_tokens)) + np.log(1 - PROB_SPAM)\n",
    "joint_log_ham[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf591d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_log_ham.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17fcebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_log_spam.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bc444",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "### Checking for the Higher Joint Probability\n",
    "\n",
    "$$ P(Spam \\, | \\, X) >  P(Ham \\, | \\, X) $$\n",
    "<br>\n",
    "<center><b> OR </b></center>\n",
    "<br>\n",
    "$$ P(Spam \\, | \\, X) <  P(Ham \\, | \\, X) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89d80b9b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for i in range(1724):\n",
    "    if joint_log_spam[i] > joint_log_ham[i]:\n",
    "        prediction.append(1)\n",
    "    elif joint_log_ham[i] > joint_log_spam[i]:\n",
    "        prediction.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdfbea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8949f744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a386b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87b1a45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d6593",
   "metadata": {},
   "source": [
    "### Simplifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "146d1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_log_spam = X_test.dot(np.log(prob_token_spam)) + np.log(PROB_SPAM)\n",
    "joint_log_ham = X_test.dot(np.log(prob_token_ham)) + np.log(1 - PROB_SPAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31304c",
   "metadata": {},
   "source": [
    "<h4> Understand: </h4>\n",
    "<p> I can do what I did above because since we are trying to predict  if an email is spam or not , that prediction won't depend on p(X) which was represented by \"- np.log(prob_all_tokens)\". And so removing that part of the code won't break the calculation </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f93c3",
   "metadata": {},
   "source": [
    "# Metrics & Evaluation\n",
    "\n",
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ada5c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of correctly classified docs. is 1685\n",
      "The total number of incorrectly classified docs. is 39\n"
     ]
    }
   ],
   "source": [
    "correct_docs = (y_test == prediction).sum()\n",
    "print(f\"The total number of correctly classified docs. is {correct_docs}\")\n",
    "numdocs_wrong = X_test.shape[0] - correct_docs\n",
    "print(f\"The total number of incorrectly classified docs. is {numdocs_wrong}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49b4e34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9773781902552204\n"
     ]
    }
   ],
   "source": [
    "# accuracy = nr. of correct predictions / total nr. of predictions\n",
    "\n",
    "accuracy = correct_docs / X_test.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a50a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
